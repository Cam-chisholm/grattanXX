\chapter{Data sources and issues} \label{chap2}

\section{Student-level NAPLAN datasets used in the report}


The analysis in \textit{Widening gaps} is based on linked student-level NAPLAN records.\footnote{Analysis was carried out for reading and numeracy, but not the other domains.} There are two major datasets used in the analysis:
\begin{itemize}
\item NAPLAN results across all four assessment domains and year levels for all Australian students recorded in 2014, linked with their 2012 results where applicable.\footcite{acara2014} This dataset contains test scores for more than one million students for each domain in 2014, and more than 700,000 in 2012.\footnote{Only students in Years 5, 7, and 9 in 2014 have a linked record in 2012. Linked records are not available for students in the Northern Territory.}
\item NAPLAN results across all four domains recorded between 2009 to 2015 for the cohort of Victorian students who were in \mbox{Year 3} in 2009.\footcite{vcaa2015} For each domain, more than 55,000 students have a \mbox{Year 3} test score and a score from at least one other test year. More than 45,000 students have a test score recorded in all of Years 3, 5, 7, and 9 for both reading and numeracy. 
\end{itemize}

Equivalent year levels are estimated using the national dataset to create a national benchmark for student progress. This benchmark is used in analysis of the linked Victorian data, which allows progress of individual students to be tracked from \mbox{Year 3} to \mbox{Year 9}. In this way, the ``years of progress'' made by particular groups of Victorian students is relative to the typical Australian student, as opposed to the typical Victorian student.\footnote{This allows the analysis to pick up Victorian-specific effects. It should be noted that, on average, Victorian students score higher than most other states. One explanation for this is that Victorian students are, on average, more likely to come from a high SES background [\textcite{acara2014}].}

The data contain a number of student background variables, including gender, parental education and occupation, language background and indigenous status. Some geographic information is available at the school level, including state, and whether the school is located in a metropolitan, regional, or rural area. The Victorian data also include the local government area of the school as well as a measure of school socioeconomic status (SES): the Index of Community Socio-Educational Advantage (ICSEA).\footnote{To prevent school identification, the Victorian ICSEA data were given to us in bands of 26 points.} The national dataset contains a randomised school-level indicator, but not possible to identify schools themselves. 

Two additional datasets tracking different cohorts of students are used to check the robustness of the analysis -- the NAPLAN results across all domains and year levels for all Australian students recorded in 2013, linked with their 2011 results, and the NAPLAN results across all domains recorded between 2008 to 2014 for the cohort of Victorian students who were in \mbox{Year 3} in 2008.\footcite{acara2013,vcaa2014} Because NAPLAN results vary across cohorts, the analysis was rerun with these data. This confirmed that the key findings of the report -- in terms of the scale and direction of learning gaps -- were not cohort-specific (see Sections \ref{sec:cohort1} and \ref{sec:cohort2}).

\section{Defining the `typical' student}

The analysis presented in \textit{Widening gaps} focuses on the `typical' student, either at the population level or within a particular sub-group of students. As noted in the main report and in \Cref{chap1}, for the purposes of measuring \textit{Years of Progress}, the typical student in a given year level is defined as the student with the median NAPLAN scale score. Analysis of particular sub-groups of students (such as those grouped by parental education or school ICSEA) is performed according to the typical student within each sub-group -- the sub-group median.

An important advantage of using the median over the mean is that it is not directly affected by outliers. For instance, there may be a number of students who do not care about NAPLAN results who leave questions unanswered on the test instead of attempting them, meaning that their NAPLAN scale scores would not be an accurate estimate of their true skill level. These inaccurate results would have a much larger impact on estimates of the mean score and the mean gain score than they would have on the median.\footnote{Estimates of the median would only be impacted in this way if a substantial number of students whose true skill level is above the median are recorded below the median as a result of leaving questions unanswered.} NAPLAN scale scores also tend to have a small positive skew (particularly for numeracy), which lifts the mean relative to the median.

\section{Indicators of parental education}

The report analyses how NAPLAN results and progress vary by different levels of parental education, using the Victorian 2009--15 dataset. While there is information on the highest schooling year attained, most parents of school-age children in Victoria have completed \mbox{Year 12}; we therefore focus on educational attainment beyond school. Students can be divided into four groups based on the highest level of post-school parental education:
\begin{itemize}
\item at or above Bachelor's degree
\item diploma
\item certificate I to IV
\item no post-school education (\mbox{Year 12} or below).
\end{itemize}
Parental education is a strong predictor of household income, and is highly correlated with other socioeconomic factors.\footnote{See, for instance, \textcite{oecd2015}.} For example, in 85 per cent of the households where a parent has a university degree, the highest level of parental occupation is either manager or professional, compared to only 21 per cent of households where neither parent has a degree or diploma.\footnote{Grattan analysis of \textcite{vcaa2015}. Some studies use a composite measure of socioeconomic status, which includes both parental education and occupation, such as \textcite{marks2015,houng2015}. Our analysis highlights the cycle of educational disadvantage, looking at the relationship between parental education and the education outcomes of students.}

\Vref{fig:parental_ed} shows that the four categories of parental education include at least 15 per cent of all students. Preliminary analysis suggests that the difference in student attainment and progress between the lowest two categories of parental education is small -- as a result, in the main report we group these into a single category: `below diploma'.

Much of the exploratory analysis in this technical report groups students by parental education.\footnote{This is only one of the ways in which we group students to analyse student progress in \textit{Widening gaps}. We explore the robustness of results by parental education for simplicity and consistency.}

\begin{figure}[t]
 \captionwithunits{Students are well represented in each category of parental education}{Percentage of students, Victorian 2009--15 cohort}
 \includegraphics[width=\columnwidth]{atlas/Parental_ed.pdf}\label{fig:parental_ed}

\source{Grattan analysis of \textcite{vcaa2015}.}
\end{figure}

\section{Using ICSEA as a measure of school socioeconomic status} \label{sec:icsea}

The report analyses how NAPLAN results and progress vary by the Index of Community Socio-Educational Advantage (ICSEA, which is referred to in the report as `school advantage') in the Victorian 2009--15 dataset. ICSEA was developed by ACARA so that NAPLAN results could be compared between schools with similar student backgrounds. The index is based on student-level factors such as parental education and employment, indigenous status, and school-level factors such as remoteness and the proportion of indigenous students.\footnote{Geographic census data are also used in the index calculation.} The index is constructed as a linear combination of these student- and school-level SES variables.

To determine the weighting applied to each variable, a regression model is estimated: average NAPLAN score (across all domains) against each SES variable. The estimated parameters of this model determine the weightings -- essentially this means that the SES variables are weighted according to how strongly they relate to NAPLAN results. This index is then averaged across all students in each school, and scaled nationally so that the ICSEA distribution has a mean of 1000 and a standard deviation of 100. This methodology provides an estimate of ICSEA for each school, which is adjusted each year.\footnote{For more detail, see \textcite{acara2014a}.}

There is a question as to whether this methodology means that we will observe a strong relationship between school ICSEA and NAPLAN results, even if the school SES variables are only weakly related to NAPLAN results. We do not believe this to be the case. While NAPLAN results are used in the construction of ICSEA, they are not used as an input variable -- ICSEA is still entirely a linear function of SES variables. That is, the strong relationship observed between ICSEA and NAPLAN results is driven by SES factors, not by the way the index is constructed.

We use the Victorian linked data to analyse the impact of school ICSEA on student progress. We allocate schools into one of three ICSEA groups:\footnote{Allocation is done for each of 2009, 2011, 2013 and 2015, since schools can change their socio-economic mix and ICSEA is recalculated by ACARA for all schools each year.}
\begin{itemize}
\item ICSEA greater than 1090 (approximately the top quartile of schools in Victoria)
\item ICSEA greater than 970 but less than 1090 (approximately the middle two quartiles of schools in Victoria) 
\item ICSEA less than 970 (approximately the bottom quartile of schools in Victoria).\footnote{These cut points were chosen from the ICSEA bands available to us. It should be noted that the average ICSEA of Victorian schools is about 30 to 40 points higher than the national average.}
\end{itemize}

These are referred to as \textit{high advantage}, \textit{medium advantage}, and \textit{low advantage} schools respectively. 

\section{Missing data} \label{sec:missing}

There are two major sources of missing NAPLAN data: non-participation in NAPLAN and results that are not linked for the same student in different years. The non-linkage of results is only an issue for students in the Northern Territory -- no linked data are available for Northern Territory in the national dataset.

For any given NAPLAN test, participation rates are high, usually exceeding 90 per cent. The most common reason for non-participation is student absenteeism. This is usually four per cent or less, but rises to seven per cent in \mbox{Year 9}, as shown for numeracy in \Cref{fig:absent_pc}. A small proportion of students (typically less than two per cent) are given an exemption from taking the NAPLAN test, usually if they have a significant disability or face a major language barrier. Finally, some students are withdrawn from testing by their parent/carer, although this is less than two per cent on almost every test.

\begin{figure}[t]
 \captionwithunits{Students are more likely to be absent from a NAPLAN test in \mbox{Year 9}}{Percentage of students that are absent from NAPLAN numeracy test, Victorian 2009--2015 cohort}
 \includegraphics[width=\columnwidth]{atlas/Absent_pc.pdf}\label{fig:absent_pc}
\notes{Does not include students who are exempt, withdrawn or miss a test due to leaving Victoria. Results are similar for reading.}

\source{Grattan analysis of \textcite{vcaa2015}.}
\end{figure}

Despite a high participation rate on each test, these missing data can potentially reduce the size of the linked samples quite significantly. In the cohort of Victorian students who took the \mbox{Year 3} test in 2009, only about 72 per cent took all four NAPLAN tests to \mbox{Year 9} for numeracy and reading. This is because different students missed the test in different years, and also because some students moved out of Victoria before \mbox{Year 9}.\footnote{There are also students that accelerated or repeated a year -- these students are included in the analysis, although some have not completed \mbox{Year 9} by 2015.}

A brief analysis suggests that students are less likely to miss a test due to being absent/withdrawn or an exemption if their parents are better educated. \Cref{fig:missing} shows that of the Victorian cohort of students in \mbox{Year 3} in 2009, 40 per cent of those whose parents have no tertiary education missed at least one test between \mbox{Year 3} and \mbox{Year 9}, compared to only 25 per cent of students where a parent has a university degree.

\begin{figure}[t]
 \captionwithunits{Students from households with higher parental education are less likely to miss one or more NAPLAN tests}{Percentage of students that miss a NAPLAN test, Victorian 2009--15 cohort}
 \includegraphics[width=\columnwidth]{atlas/Missing_SES.pdf}\label{fig:missing}
\notes{Includes all Victorian students in \mbox{Year 3} in 2009, and all NAPLAN tests taken up to 2015. `Absent from at least one test' includes those who were withdrawn, and those not in Victoria in one or more test-taking years after \mbox{Year 3}. Students that have been both absent and exempt from tests are categorised as exempt.}

\source{Grattan analysis of \textcite{vcaa2015}.}
\end{figure}

Given that students of well-educated parents typically score higher and make higher gains from a given starting score than those whose parents are less well educated, the consequence of ignoring missing data is an upwards bias in estimates of the median score and median gain score.\footnote{That is, the estimated median is likely to be above the actual population 50th percentile.}

It is also possible that students who miss a test would have made a lower gain score than other students, even after controlling for starting score. With only two years of linked data it would not be possible to test this. But with four years of linked data, as is available with the Victorian 2009 to 2015 cohort, there are students that have missed a test in one or two years, but for whom we observe NAPLAN scale scores in at least two other years. \Vref{fig:missing_gain} shows the estimated median gain score in reading between Years 5 and 7 for students that did not miss a test in any year, and for students that missed a test in \mbox{Year 3}, \mbox{Year 9} or both. Not only are those that missed a test predicted to make smaller gains, but the gap is larger for students whose parents do not have a degree or diploma.

\begin{figure}[t]
 \captionwithunits{Missing data have more of an impact on gain scores for students from less-educated households}{Median NAPLAN gain score by highest level of parental education, reading, \mbox{Year 5} to \mbox{Year 7}, Victorian 2009--15 cohort}
 \includegraphics[width=\columnwidth]{atlas/Missing_gain.pdf}\label{fig:missing_gain}
\notes{`Missing' includes all students that were absent/withdrawn from either the \mbox{Year 3} or \mbox{Year 9} reading test, but does not include exempt students. `Non-missing' includes all students that did not miss a single NAPLAN test. A similar pattern exists for numeracy, for other year levels, and for school advantage.}

\source{Grattan analysis of \textcite{vcaa2015}.}
\end{figure}

This means that estimates of median progress for particular sub-groups are likely to be upwards biased if missing data are ignored. But the bias is likely to be much larger for lower levels of parental education. In turn, this means the gap in student progress calculated between students with high and low parental education is likely to be underestimated rather than overestimated.\footnote{The report shows a very consistent pattern of students from well-educated households out-performing those from lower-educated households in \mbox{Year 3}, and this gap growing over time. A similar pattern is found between high and low advantaged schools. These are key findings of the report. If missing data could be adequately taken into account, it is likely that these gaps would be estimated to be even larger.}

Our analysis of NAPLAN gain scores does not impute missing results. Students who are given an exemption from one or more tests are excluded from the analysis.\footnote{For the purposes of reporting, ACARA assume exempt students are performing below the national minimum standard. Imputing NAPLAN scale scores for these students would change the sample median, but with so few students exempt it is unlikely the results would change significantly.} When estimating progress for Victorian students, we aim to minimise bias -- rather than excluding all students that miss a test, we include all students that undertook the \mbox{Year 3} test and at least one other test. This approach is outlined in more detail in Section \ref{sec:ses_sub}.

\section{Measurement error and bias}

\subsection{Measurement error at the student level}

The NAPLAN scale score that a student receives for a particular test is known as a `weighted likelihood estimate' (WLE).\footnote{These are also referred to as `Warm's Estimates'; see \textcite{warm1989}.} Two students that answer the same number of correct answers on the same test receive the same WLE.

The score that a student receives on the NAPLAN test provides an estimate of their true skill level in a particular domain, but this is subject to substantial measurement error. The accuracy of the estimate increases with the number of questions asked.\footnote{On the \mbox{Year 3} numeracy test in 2009, for instance, there are 35 questions, and NAPLAN scale scores are estimated with a standard error between 24 and 35 for the vast majority of students. On the \mbox{Year 9} numeracy test in 2015, there are 64 questions, and the standard error of NAPLAN scale scores is between 17 and 30 for nearly all students. Extreme scores (nearly all questions correct/incorrect) are estimated with much higher standard errors [\textcite{acara2015d}].} Two scores are needed to estimate progress over time, and each is subject to measurement error. It is therefore difficult to accurately estimate the progress of an individual student using NAPLAN.

NAPLAN results are more accurate for estimating the progress of a sizeable group of students, as measurement error is reduced when results are aggregated across students. But simply aggregating does not solve all of the potential measurement error issues. This section outlines these issues in detail and explains the approach we have taken to mitigate them.\footnote{There may also be measurement error issues in other variables -- for instance, parental education may change over the course of a child's schooling years, but this is not recorded. Our analysis assumes that the recording of background variables is accurate.}

\subsection{Using NAPLAN scale scores (WLEs) may result in imprecise estimates of progress} \label{sec:rttm}

\subsubsection*{Skill level is continuous, but NAPLAN scale scores are discrete}

NAPLAN scale scores provide an estimate of student skill level, a continuous latent variable. But because there are a finite number of questions on each NAPLAN test, the estimates of student skill level (NAPLAN scale scores) have a discrete distribution.

On the \mbox{Year 3} numeracy test, for example, there are only 35 questions, meaning that there are only 35 possible NAPLAN scale scores a student can receive. The cohort of students that takes the test in 2014 would receive a different set of scores to the cohort taking the test in 2015, even where there is no significant difference between the two cohorts.\footnote{A histogram comparing two cohorts would show a similar overall distribution, but the estimated points on the NAPLAN scale would be different. It is therefore important to take care when interpreting results across students from different cohorts.} Ignoring the discrete nature of the estimates could overstate the difference between two cohorts because of `edge effects', especially when comparing performance in terms of percentiles, such as the progress or achievement of the median student.

%\newpage
\subsubsection*{Regression to the mean}

In the context of comparing student progress over two or more NAPLAN tests, \textit{regression to the mean} suggests that an extreme NAPLAN score in one year (either extremely low or high) is likely to be followed by a less extreme score on the following test (two years later). This is not because students at the extremes are making significantly high or low progress, but because the original test score is exaggerated by measurement error. This may lead to learning progress being significantly overstated by gain scores for students who start with a very low score, and understated for students who start with a very high score.\footnote{The data show a systematic pattern of high gain scores for low prior scores and low gain scores for high prior scores; see, for example, \Vref{fig:gain_prior} and \Vref{fig:missing_gain}. But if this were entirely due to regression to the mean, we would expect the path of progress for the median student from \mbox{Year 3} to \mbox{Year 9} to be approximately linear -- this is clearly not the case.}

\textcite{wu2005} notes that the average of the WLEs provides an unbiased estimate of the population mean skill level, but the sample variance overstates the population variance. This bias disappears as the number of test questions increases. For students who score close to the mean, the bias in the WLE as an estimate of their skill level will be small. But for extreme percentiles, the bias can be large.\footnote{A way to think about this is that the effective number of questions declines as student skill level moves further from the level at which the test is set. For example, a student at the 90th percentile will find most questions too easy, while a student at the 10th percentile will find most questions too difficult. Only a few questions will be set at an appropriate level for such students. The move to NAPLAN online will allow better targeting of questions, reducing the measurement error at the extremes.}

It is important to note that an extreme score for a particular sub-group might not be an extreme score for another sub-group. For example, the NAPLAN scale score equal to the 95th percentile in \mbox{Year 7} numeracy for those whose parents have no post-school qualifications is only at the 82nd percentile for those who have a parent with a university degree. This means that the regression to the mean between the \mbox{Year 7} and \mbox{Year 9} test is likely to be stronger for a high achieving student whose parents have no post-school qualifications than it is for a high achieving student with a university-educated parent.\footnote{Just because a student does not have a university-educated parent, this does not mean that a high NAPLAN scale score is overstating their true skill level. But when we compare two students with the same high score, one with a university-educated parent and one without, the one without is more likely to have had an unusually good test day (i.e. scoring above their true skill level) than the student with a university-educated parent.}

\subsection{Approaches to mitigate the impact of measurement error and bias} \label{sec:pv}

\subsubsection*{Simulation approach}

All WLEs (NAPLAN scale scores) are point estimates and are associated with a standard error. \textcite{warm1989} shows that these estimates are asymptotically normally distributed. Using this property, we approximate the distribution of student skill level, $\theta$, given these estimates:
\begin{equation}
\theta_{n} \overset{a}\sim \mathcal{N}\left(\widehat{\mu}_{n},\widehat{\sigma}_{n}^{2} \right)
\end{equation}
where $n$ is the number of questions correctly answered, $\widehat{\mu}_{n}$ is the corresponding WLE, and $\widehat{\sigma}_{n}^{2}$ is the variance of the WLE. 

For each student, we simulate a NAPLAN scale score (skill level) as a random draw from this distribution.\footnote{This is performed for each year in the Victorian cohort and each year in the national dataset, using the standard errors reported by \textcite{acara2015d}.} This creates a sample that has the properties of a continuous distribution, allowing for more accurate estimates of percentiles.

\Cref{fig:histogram} compares a histogram of discrete NAPLAN scale scores to a histogram of simulated NAPLAN scale scores. While this approach does not remove measurement error at the individual student level, it takes into account that measurement error varies across students with different scores.

\begin{figure}[H]
 \captionwithunits{The simulation approach solves the issue of discrete NAPLAN scale scores}{Histogram of \mbox{Year 5} NAPLAN scale score, numeracy, Australia}
 \includegraphics[width=\columnwidth]{atlas/Hist.pdf}\label{fig:histogram}
  \includegraphics[width=\columnwidth,page=2]{atlas/Hist.pdf}
\notes{Frequency is not shown on Y-axes, but scaled so that both charts can be compared. Bin width = 0.5.}

\source{Grattan analysis of \textcite{acara2014}.}
\end{figure}


\newpage
\subsubsection*{Use of sub-groups with large samples}

Simulating NAPLAN scores does not remove measurement error at the individual student level. In fact, it increases the standard error associated with an individual student estimate and gain score.\footnote{This approach would be inappropriate for reporting individual student results.} We keep this measurement error to a minimum by aggregating students into sub-groups that have large samples, and calculating our results based on multiple random draws.\footnote{Sub-groups analysed typically have between 7000 and 25,000 students. The standard error due to measurement in a sub-group is proportional to $\sqrt{n}$, the square root of the sub-group sample size. For a sub-group with 10,000 people, the standard error will be 100 times smaller than it will be for an individual student.}

\subsubsection*{Avoiding extreme percentiles}

There is no straightforward way to estimate the magnitude of the bias in the WLEs for different percentiles. But it is well known that the magnitude of the bias due to regression to the mean is largest for extreme percentiles, and that the bias is small for percentiles close to the median. The impact of regression to the mean is also larger when the correlation between two measurements (such as test scores) is weak. In our sample, the correlation between NAPLAN test scores across two test-taking years for a given domain is between 0.75 and 0.8 -- this strong correlation suggests regression to the mean will have only a small impact for most percentiles.

Nonetheless, our analysis aims to avoid estimating NAPLAN scale scores and gain scores for students at extreme percentiles, and most analysis is focused around the median student. We use a rule of thumb to minimise bias due to regression to the mean -- no analysis is based on the estimated NAPLAN scale score or gain score of students below the 10th percentile or above the 90th percentile.\footnote{These extreme percentiles are avoided both for the overall population, and for particular sub-groups.}

In constructing the benchmark curve to estimate equivalent year levels (outlined in \Vref{chap3}), it is necessary to estimate the median gain score of below-average students from Years 3 to 5, and above-average students from Years 7 to 9. It is possible to estimate the NAPLAN scale score for a student as low as 18 months behind \mbox{Year 3} level, and as high as three years ahead of \mbox{Year 9} level without using extreme percentiles.

For the analysis of progress using Victorian data, we track low, medium, and high achieving students based on their percentile at \mbox{Year 3} -- the 20th, 50th, and 80th of the Victorian population. But these percentiles can be more extreme when analysing sub-groups. In \mbox{Year 3} numeracy, for example, the 20th percentile across the population is equal to the 12th percentile for students who have a parent with a university degree, and the 80th percentile at the population level is the 87th percentile when the highest level of parental education is below a diploma. \Cref{tab:percentiles} shows the within-group percentiles for the 20th and 80th percentiles in \mbox{Year 3} at the population level. Using these percentiles at the population level ensures that we do not go below the 10th or exceed the 90th percentile for any parental education sub-group.\footnote{This also holds for school advantage.}

The gaps in progress between high and low parental education levels may still be overstated due to regression to the mean, particular when comparing from the 20th or the 80th percentile in \mbox{Year 3}. This is explored more in Section \ref{sec:reg_mean_gaps}.

\newpage
\begin{table}[htbp]
  \centering
  \captionwithunits{Using the 20th and 80th percentiles at the population level avoids extreme percentiles within sub-groups}{Within-group percentile in \mbox{Year 3} numeracy by parental education, Victorian 2009--15 cohort}
    \begin{tabular}{lcc}

 Sub-group & \multicolumn{2}{c}{Percentile} \\ \cmidrule(lr){1-1}
\cmidrule(lr){2-3}
    \textit{Population} & \textit{20}   & \textit{80} \\
    Degree or above & 11.9  & 70.8 \\
    Diploma & 19.7    & 81.5 \\
    Below diploma & 26.3    & 86.7 \\
    \bottomrule
    \end{tabular}%
  \label{tab:percentiles}%
\begin{flushleft}\notes{`Extreme percentiles' defined as below 10th or above 90th within a sub-group.}

\source{Grattan analysis of \textcite{vcaa2015}.}\end{flushleft}
\end{table}%
\vspace{-18pt}

\subsubsection*{Reporting of results and standard errors}

To simplify the presentation of our findings, the report does not show standard errors on point estimates of NAPLAN scale scores or equivalent year levels. But confidence bounds are estimated to ensure the significance of reported results. We calculate 99 per cent confidence intervals using a bootstrap approach with 200 replications, each with a different set of random draws.\footnote{The lower bound of each confidence interval is estimated as the average of the two smallest bootstrap point estimates, while the upper bound is estimated as the average of the two largest bootstrap point estimates.} Separate bootstrap simulations are run for estimation of the benchmark curve with the national dataset and for estimation of student progress using the Victorian dataset.

We estimate a confidence interval for the benchmark equivalent year level curve, as well as confidence intervals for the analysis of progress using the Victorian cohort. For results that are reported in terms of equivalent year levels or years of progress, these confidence intervals are calculated using both bootstrap simulations.\footnote{Each replication from one simulation is linked to a replication from the other. This approach takes into account the measurement error in the Victorian cohort, as well as the measurement error in the estimation of equivalent year levels.} 

The confidence intervals are used to validate the significance of our findings -- we do not draw conclusions from any results that are not statistically significant (at the 1 per cent level).

\subsubsection*{Plausible values}

The best approach to reduce the impact of measurement error is to use \textit{plausible values}. Like the simulation approach outlined above, this approach would simulate a NAPLAN scale score from a continuous distribution for each student, including imputing values for missing data. But plausible values are simulated from a distribution that takes into account student and school background factors.\footnote{In theory these could also take into account NAPLAN scores in other year levels.} NAPLAN reports produced by ACARA are based on analysis using plausible values.\footcite[][22]{acara2015a}

When simulated correctly, plausible values are able to produce unbiased estimates of percentiles and gain scores for each sub-group.\footcite{wu2005} Plausible values were available for the 2014 test year in the national dataset, but not for the 2012 results or the Victorian 2009--15 cohort. This means we did not have the data to use plausible values to analyse progress.\footnote{In any case, the 2014 plausible values are, to the best of our knowledge, generated independently of prior test scores. Analysing student progress would ideally be done using plausible values simulated from a distribution that takes both prior and subsequent test scores into account.}

We do, however, utilise the 2014 plausible values (generated by ACARA) for estimating the population distribution of results for each year level. These estimates therefore take missing data and measurement error into account. 